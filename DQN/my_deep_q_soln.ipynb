{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "\n",
    "from lib import plotting\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-28 17:10:18,230] Making new env: Breakout-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.envs.make(\"Breakout-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Atari actions\" 0 (no op), 1 (fire), 2 (left), 3 (right)\n",
    "VALID_ACTIONS = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(VALID_ACTIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StateProcessor(object):\n",
    "    \"\"\"A class to process an atari image for input into the nn\"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph.\n",
    "        with tf.variable_scope('state_processor'):\n",
    "            # Input is a 210 x 160 x 3 array describing the screen\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            # Transform to grayscale\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            # Crop top and bottom of image to make it square\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output,\n",
    "                                                        34, 0, 160, 160)\n",
    "            # resize image to 84 x 84\n",
    "            self.output = tf.image.resize_images(self.output, [84, 84],\n",
    "                                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "    \n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        process an atari image\n",
    "        \n",
    "        Args:\n",
    "            sess: A TensorFlow session object\n",
    "            state: A [210, 160, 3] Atari RGB state\n",
    "            \n",
    "        Returns:\n",
    "            A processed [84, 84, 1] state representing grayscale values\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, {self.input_state: state})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Estimator(object):\n",
    "    \"\"\"Neural Network to estimate Q-Value\n",
    "    \n",
    "    This network is used for both the Q-Network and Target Network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scope='estimator', summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        # Write Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph\n",
    "        \"\"\"\n",
    "        \n",
    "        # Placeholders for input\n",
    "        # Input are 4 RGB frames of shape 160, 160 each\n",
    "        self.X_pl = tf.placeholder(shape=[None, 84, 84, 4], \n",
    "                                   dtype=tf.uint8,\n",
    "                                   name='X')\n",
    "        # TD Target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name='y')\n",
    "        # Integer if of which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None],\n",
    "                                         dtype=tf.int32,\n",
    "                                         name='actions')\n",
    "        # Variables\n",
    "        layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "                [8, 8, 4, 32], stddev=0.1))\n",
    "        layer1_biases = tf.Variable(tf.zeros([32]))\n",
    "        \n",
    "        layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "                [4, 4, 32, 64], stddev=0.1))\n",
    "        layer2_biases = tf.Variable(tf.zeros([64]))\n",
    "        \n",
    "        layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "                [3, 3, 64, 64], stddev=0.1))\n",
    "        layer3_biases = tf.Variable(tf.zeros([64]))\n",
    "        \n",
    "        layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "                [7*7*64, 512], stddev=0.1))\n",
    "        layer4_biases = tf.Variable(tf.zeros([512]))\n",
    "        \n",
    "        layer5_weights = tf.Variable(tf.truncated_normal(\n",
    "                [512, len(VALID_ACTIONS)], stddev=0.1))\n",
    "        layer5_biases = tf.Variable(tf.zeros([len(VALID_ACTIONS)]))\n",
    "        \n",
    "        \n",
    "        X = tf.to_float(self.X_pl) / 255.0\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        # Three convolutional layers\n",
    "        conv = tf.nn.conv2d(X, layer1_weights, [1, 4, 4, 1],  padding='VALID', name='conv1')\n",
    "        relu = tf.nn.relu(conv + layer1_biases, name='relu1')\n",
    "        conv = tf.nn.conv2d(relu, layer2_weights, [1, 2, 2, 1], padding='VALID', name='conv2')\n",
    "        relu = tf.nn.relu(conv + layer2_biases, name='relu2')\n",
    "        conv = tf.nn.conv2d(relu, layer3_weights, [1, 1, 1, 1], padding='VALID', name='conv3')\n",
    "        relu = tf.nn.relu(conv + layer3_biases, name='relu3')\n",
    "        shape = relu.get_shape().as_list()\n",
    "        \n",
    "        reshape = tf.reshape(relu, [-1, shape[1]*shape[2]*shape[3]])\n",
    "        fc1 = tf.matmul(reshape, layer4_weights) + layer4_biases\n",
    "        self.predictions = tf.matmul(fc1, layer5_weights) + layer5_biases\n",
    "        \n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1]\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "        \n",
    "        # Loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "        \n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, \n",
    "                                                global_step=tf.contrib.framework.get_global_step())\n",
    "        \n",
    "        # Summaries for Tensorboard\n",
    "        self.summaries = tf.summary.merge([\n",
    "                tf.summary.scalar('loss', self.loss),\n",
    "                tf.summary.histogram('loss_hist', self.losses),\n",
    "                tf.summary.histogram('q_values_hist', self.predictions),\n",
    "                tf.summary.scalar('max_q_value', tf.reduce_max(self.predictions))\n",
    "            ])\n",
    "        \n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"predictions action values.\n",
    "        \n",
    "        Args:\n",
    "            sess: Tensorflow session\n",
    "            s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "                \n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing\n",
    "            the action values.\n",
    "        \"\"\"\n",
    "        A = sess.run(self.predictions, {self.X_pl: s})\n",
    "        \n",
    "        return A\n",
    "        #return sess.run(self.predictions, {self.X_pl: s})\n",
    "        \n",
    "    def update(self, sess, s, a, y):\n",
    "        \"\"\"\n",
    "        UPdate the estimator toward the given targets\n",
    "        \n",
    "        Args:\n",
    "            sess: Tensorflow session object\n",
    "            s: State input of size [batch_size, 4, 160, 160, 3]\n",
    "            a: Chosen action of size [batch_size]\n",
    "            y: Targets of shape [batch_size]\n",
    "            \n",
    "        Returns:\n",
    "            The calculated loss of each batch\n",
    "        \"\"\"\n",
    "        feed_dict = {self.X_pl: s, self.y_pl: y, self.actions_pl: a}\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, \n",
    "             tf.contrib.framework.get_global_step(), \n",
    "             self.train_op,\n",
    "             self.loss], feed_dict=feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -2.5180614   -0.19548371  10.95501137   0.25467342]\n",
      " [ -2.5180614   -0.19548371  10.95501137   0.25467342]]\n",
      "156.702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cully/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# For Testing\n",
    "\n",
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "e = Estimator(scope='test')\n",
    "sp = StateProcessor()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    observation = env.reset()\n",
    "    \n",
    "    observation_p = sp.process(sess, observation)\n",
    "    observation = np.stack([observation_p] * 4, axis=2)\n",
    "    observations = np.array([observation] * 2)\n",
    "    \n",
    "    print(e.predict(sess, observations))\n",
    "    \n",
    "    # Test training step\n",
    "    y = np.array([10.0, 10.0])\n",
    "    a = np.array([1, 3])\n",
    "    print(e.update(sess, observations, a, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def copy_model_parameters(sess, estimator1, estimator2):\n",
    "    \"\"\"\n",
    "    Copies the model params of one estimator to another\n",
    "    \n",
    "    Args:\n",
    "        sess: Tensorflow sesion instance\n",
    "        estimator1: Estimator to copy the parameters from\n",
    "        estimator2: Estimator to copy the parameters to\n",
    "    \"\"\"\n",
    "    e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "    e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "    e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "    e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "    \n",
    "    update_ops = []\n",
    "    for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "        op = e2_v.assign(e1_v)\n",
    "        update_ops.append(op)\n",
    "    sess.run(update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximation\n",
    "    \n",
    "    Args:\n",
    "        esimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment\n",
    "        \n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument\n",
    "        and returns the probabilites for each action in the form of a \n",
    "        numpy array of length nA\n",
    "    \"\"\"\n",
    "    \n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "       \n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        \n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def deep_q_learning(sess, \n",
    "                    env, \n",
    "                    q_estimator, \n",
    "                    target_estimator, \n",
    "                    state_processor, \n",
    "                    num_episodes, \n",
    "                    experiment_dir, \n",
    "                    replay_memory_size=500000, \n",
    "                    replay_memory_init_size=50000, \n",
    "                    update_target_estimator_every=10000, \n",
    "                    discount_factor=0.99, \n",
    "                    epsilon_start=1.0, \n",
    "                    epsilon_end=0.1, \n",
    "                    epsilon_decay_steps=500000, \n",
    "                    batch_size=32, \n",
    "                    record_video_every=50):\n",
    "    \"\"\"\n",
    "    Q-learning algorithim for off-policy TD control using function approximation\n",
    "    Finds the optimal greedy policy while following epsilon-greedy one\n",
    "    \n",
    "    Args:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: A StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sampel when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Lambda time discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        record_video_every: Record a video every N episodes\n",
    "\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \n",
    "    \"\"\"\n",
    "    Transition = namedtuple('Transition', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "    \n",
    "    # The replay memory\n",
    "    replay_memory = deque(maxlen=replay_memory_size)\n",
    "    \n",
    "    # Keeps track of useful stats\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "    \n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, 'checkpoints')\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'model')\n",
    "    monitor_path = os.path.join(experiment_dir, 'monitor')\n",
    "    \n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "        \n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    \n",
    "    # Get the current time step\n",
    "    total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "    \n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        q_estimator,\n",
    "        len(VALID_ACTIONS))\n",
    "    \n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(sess, state)\n",
    "    state = np.stack([state] * 4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        if i % 500 == 0:\n",
    "            print(\"Step {}\".format(i))\n",
    "        action_probs = policy(sess, state, epsilons[total_t])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "        next_state = state_processor.process(sess, next_state)\n",
    "        next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state_processor.process(sess, state)\n",
    "            state = np.stack([state] * 4, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "    \n",
    "    # Record videos\n",
    "    env.monitor.start(monitor_path,\n",
    "                     resume=True,\n",
    "                     video_callable=lambda count: count % record_video_every)\n",
    "    \n",
    "    for i_episode in xrange(num_episodes):\n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "        \n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(sess, state)\n",
    "        state = np.stack([state]*4, axis=2)\n",
    "        loss = None\n",
    "        \n",
    "        # One step in the environment.\n",
    "        for t in itertools.count():\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "            \n",
    "            # Add epsilon to Tensorboard\n",
    "            episode_summary = tf.Summary()\n",
    "            episode_summary.value.add(simple_value=epsilon, tag='epsilon')\n",
    "            q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "            \n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                print(\"Copying parameters from q_estimator to target...\\n\")\n",
    "                copy_model_parameters(sess, q_estimator, target_estimator)\n",
    "            \n",
    "            \n",
    "            # Print step we're on, useful for debugging\n",
    "            if t % 100 == 0:\n",
    "                print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                t, total_t, i_episode+1, num_episodes, loss))\n",
    "                sys.stdout.flush()\n",
    "            \n",
    "            # Take a step in the environment\n",
    "            action_probs = policy(sess, state, epsilon)\n",
    "            action = np.random.choice(len(action_probs), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "            next_state = state_processor.process(sess, next_state)\n",
    "            next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "            \n",
    "            transition = Transition(state, action, reward, next_state, done)\n",
    "            # If our replay_memory is full, pop the first element\n",
    "            replay_memory.append(transition)\n",
    "            \n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            \n",
    "            #Sample minibatch from the replay memory\n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "            states_batch, action_batch, reward_batch, next_state_batch, done_batch = map(\n",
    "            np.array, zip(*samples))\n",
    "            \n",
    "            # Calculate q-values and targets\n",
    "            q_values_next = target_estimator.predict(sess, next_state_batch)\n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)\n",
    "            \n",
    "            # Perform gradient descent update\n",
    "            states_batch = np.array(states_batch)\n",
    "            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "            \n",
    "            \n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode],\n",
    "                                     node_name='episode_reward', tag='episode_reward')\n",
    "        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode],\n",
    "                                     node_name='episode_length', tag='episode_length')\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "        q_estimator.summary_writer.flush()\n",
    "            \n",
    "        yield total_t, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "        \n",
    "    env.monitor.close()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model checkpoint /home/cully/git/reinforcement-learning/DQN/experiments/Breakout-v0/checkpoints/model...\n",
      "\n",
      "Populating replay memory...\n",
      "Step 0\n",
      "Step 500\n",
      "Step 1000\n",
      "Step 1500\n",
      "Step 2000\n",
      "Step 2500\n",
      "Step 3000\n",
      "Step 3500\n",
      "Step 4000\n",
      "Step 4500\n",
      "Step 0 (4162) @ Episode 1/10, loss: None\n",
      "Step 100 (4262) @ Episode 1/10, loss: 0.0784096419811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-28 17:11:48,037] Starting new video recorder writing to /home/cully/git/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.16383.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (4337) @ Episode 2/10, loss: None\n",
      "Step 100 (4437) @ Episode 2/10, loss: 0.0693007782102\n",
      "Step 200 (4537) @ Episode 2/10, loss: 0.0229906775057\n",
      "\n",
      "Episode Reward: 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-28 17:12:06,445] Starting new video recorder writing to /home/cully/git/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.16383.video000002.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 (4631) @ Episode 3/10, loss: None\n",
      "Step 100 (4731) @ Episode 3/10, loss: 0.00740694673732\n",
      "Step 200 (4831) @ Episode 3/10, loss: 0.0385140702128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-28 17:12:22,754] Starting new video recorder writing to /home/cully/git/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.16383.video000003.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode Reward: 2.0\n",
      "Step 0 (4895) @ Episode 4/10, loss: None\n",
      "Step 100 (4995) @ Episode 4/10, loss: 0.0294174849987\n",
      "Step 200 (5095) @ Episode 4/10, loss: 0.0427033752203\n",
      "Step 300 (5195) @ Episode 4/10, loss: 1.06958556175\n",
      "Step 400 (5295) @ Episode 4/10, loss: 0.013988243416\n",
      "\n",
      "Episode Reward: 7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-28 17:12:47,383] Starting new video recorder writing to /home/cully/git/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.16383.video000004.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 (5297) @ Episode 5/10, loss: None\n",
      "Step 100 (5397) @ Episode 5/10, loss: 0.0237196218222\n",
      "Step 200 (5497) @ Episode 5/10, loss: 0.0254188049585\n",
      "Step 300 (5597) @ Episode 5/10, loss: 0.0137845370919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-28 17:13:07,848] Starting new video recorder writing to /home/cully/git/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.16383.video000005.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode Reward: 3.0\n",
      "Step 0 (5631) @ Episode 6/10, loss: None\n",
      "Step 100 (5731) @ Episode 6/10, loss: 0.0465595908463\n",
      "Step 200 (5831) @ Episode 6/10, loss: 1.44431376457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-28 17:13:22,763] Starting new video recorder writing to /home/cully/git/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.16383.video000006.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode Reward: 1.0\n",
      "Step 0 (5868) @ Episode 7/10, loss: None\n",
      "Step 100 (5968) @ Episode 7/10, loss: 0.0165252201259\n",
      "Step 200 (6068) @ Episode 7/10, loss: 0.0186451822519\n",
      "Step 300 (6168) @ Episode 7/10, loss: 0.0284429229796\n",
      "\n",
      "Episode Reward: 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-28 17:13:41,368] Starting new video recorder writing to /home/cully/git/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.16383.video000007.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 (6169) @ Episode 8/10, loss: None\n",
      "Step 100 (6269) @ Episode 8/10, loss: 0.963227391243\n",
      "Step 200 (6369) @ Episode 8/10, loss: 0.0580120682716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-28 17:13:54,408] Starting new video recorder writing to /home/cully/git/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.16383.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode Reward: 1.0\n",
      "Step 0 (6379) @ Episode 9/10, loss: None\n",
      "Step 100 (6479) @ Episode 9/10, loss: 0.928222060204\n",
      "Step 200 (6579) @ Episode 9/10, loss: 0.0113477241248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-28 17:14:08,265] Starting new video recorder writing to /home/cully/git/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.16383.video000009.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode Reward: 1.0\n",
      "Step 0 (6602) @ Episode 10/10, loss: None\n",
      "Step 100 (6702) @ Episode 10/10, loss: 0.0150168975815\n",
      "Step 200 (6802) @ Episode 10/10, loss: 0.0425222814083\n",
      "Step 300 (6902) @ Episode 10/10, loss: 0.0137986261398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-28 17:14:27,163] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/cully/git/reinforcement-learning/DQN/experiments/Breakout-v0/monitor')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode Reward: 2.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'episode_rewards'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-aaa82dd4c06f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m                                    \u001b[0mreplay_memory_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                                    replay_memory_init_size=5000):\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEpisode Reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'episode_rewards'"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoint and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "\n",
    "# Create a global_step_variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "# Create estimators\n",
    "q_estimator = Estimator(scope='q', summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope='target_q')\n",
    "\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                                   env,\n",
    "                                   q_estimator=q_estimator,\n",
    "                                   target_estimator=target_estimator,\n",
    "                                   state_processor=state_processor,\n",
    "                                    num_episodes=10,\n",
    "                                   experiment_dir=experiment_dir,\n",
    "                                   replay_memory_size=50000,\n",
    "                                   replay_memory_init_size=5000):\n",
    "        print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  1.,  3.,  0.,  0.,  0.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n",
      "(84, 84)\n",
      "(84, 84, 4)\n",
      "(210, 160, 3)\n",
      "(84, 84)\n",
      "(84, 84, 3)\n",
      "(84, 84, 4)\n",
      "Transition(state=array([[[142, 142, 142, 142],\n",
      "        [142, 142, 142, 142],\n",
      "        [142, 142, 142, 142],\n",
      "        ..., \n",
      "        [142, 142, 142, 142],\n",
      "        [142, 142, 142, 142],\n",
      "        [142, 142, 142, 142]],\n",
      "\n",
      "       [[142, 142, 142, 142],\n",
      "        [142, 142, 142, 142],\n",
      "        [142, 142, 142, 142],\n",
      "        ..., \n",
      "        [142, 142, 142, 142],\n",
      "        [142, 142, 142, 142],\n",
      "        [142, 142, 142, 142]],\n",
      "\n",
      "       [[142, 142, 142, 142],\n",
      "        [142, 142, 142, 142],\n",
      "        [142, 142, 142, 142],\n",
      "        ..., \n",
      "        [142, 142, 142, 142],\n",
      "        [142, 142, 142, 142],\n",
      "        [142, 142, 142, 142]],\n",
      "\n",
      "       ..., \n",
      "       [[142, 142, 142, 142],\n",
      "        [142, 142, 142, 142],\n",
      "        [142, 142, 142, 142],\n",
      "        ..., \n",
      "        [142, 142, 142, 142],\n",
      "        [142, 142, 142, 142],\n",
      "        [142, 142, 142, 142]],\n",
      "\n",
      "       [[127, 127, 127, 127],\n",
      "        [127, 127, 127, 127],\n",
      "        [127, 127, 127, 127],\n",
      "        ..., \n",
      "        [110, 110, 110, 110],\n",
      "        [110, 110, 110, 110],\n",
      "        [110, 110, 110, 110]],\n",
      "\n",
      "       [[127, 127, 127, 127],\n",
      "        [127, 127, 127, 127],\n",
      "        [127, 127, 127, 127],\n",
      "        ..., \n",
      "        [110, 110, 110, 110],\n",
      "        [110, 110, 110, 110],\n",
      "        [110, 110, 110, 110]]], dtype=uint8), action=3, reward=0.0, next_state=array([[[142, 142, 142, 142],\n",
      "        [142, 142, 142, 142],\n",
      "        [142, 142, 142, 142],\n",
      "        ..., \n",
      "        [142, 142, 142, 142],\n",
      "        [142, 142, 142, 142],\n",
      "        [142, 142, 142, 142]],\n",
      "\n",
      "       [[142, 142, 142, 142],\n",
      "        [142, 142, 142, 142],\n",
      "        [142, 142, 142, 142],\n",
      "        ..., \n",
      "        [142, 142, 142, 142],\n",
      "        [142, 142, 142, 142],\n",
      "        [142, 142, 142, 142]],\n",
      "\n",
      "       [[142, 142, 142, 142],\n",
      "        [142, 142, 142, 142],\n",
      "        [142, 142, 142, 142],\n",
      "        ..., \n",
      "        [142, 142, 142, 142],\n",
      "        [142, 142, 142, 142],\n",
      "        [142, 142, 142, 142]],\n",
      "\n",
      "       ..., \n",
      "       [[142, 142, 142, 142],\n",
      "        [142, 142, 142, 142],\n",
      "        [142, 142, 142, 142],\n",
      "        ..., \n",
      "        [142, 142, 142, 142],\n",
      "        [142, 142, 142, 142],\n",
      "        [142, 142, 142, 142]],\n",
      "\n",
      "       [[127, 127, 127, 127],\n",
      "        [127, 127, 127, 127],\n",
      "        [127, 127, 127, 127],\n",
      "        ..., \n",
      "        [110, 110, 110, 110],\n",
      "        [110, 110, 110, 110],\n",
      "        [110, 110, 110, 110]],\n",
      "\n",
      "       [[127, 127, 127, 127],\n",
      "        [127, 127, 127, 127],\n",
      "        [127, 127, 127, 127],\n",
      "        ..., \n",
      "        [110, 110, 110, 110],\n",
      "        [110, 110, 110, 110],\n",
      "        [110, 110, 110, 110]]], dtype=uint8), done=False)\n"
     ]
    }
   ],
   "source": [
    "Transition = namedtuple('Transition', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "state = env.reset()\n",
    "print state.shape\n",
    "with tf.Session() as sess:\n",
    "    state = sp.process(sess, state)\n",
    "    #print(state)\n",
    "    print(state.shape)\n",
    "    state = np.stack([state]*4, axis=2)\n",
    "    print(state.shape)\n",
    "    action = np.random.randint(4)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    print(next_state.shape)\n",
    "    next_state = sp.process(sess, next_state)\n",
    "    print(next_state.shape)\n",
    "    print(state[:,:,1:].shape)\n",
    "    next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "    print(next_state.shape)\n",
    "    trans = Transition(state, action, reward, next_state, done)\n",
    "    print(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
